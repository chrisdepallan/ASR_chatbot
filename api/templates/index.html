<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ASR Chatbot</title>
    <link rel="stylesheet" href="{{ url_for('static', path='styles.css') }}">
    <script src="https://unpkg.com/wavesurfer.js@7/dist/wavesurfer.min.js"></script>
</head>
<body>
    <h1 style="text-align: center; color: #383351; margin-bottom: 20px;">User-centric Speech Tool</h1>

    <div class="chat-container">
        <div id="chatMessages"></div>
    </div>
    <div class="controls">
        <button id="recordButton">Record</button>
        <div id="waveform"></div>
        <button id="playButton">Play</button>
        <button id="transcribeButton" style="display: none;">
            
            <span class="arrow-icon"></span>
        </button>
    </div>
    <script>

        let mediaRecorder;
        let audioChunks = [];
        const recordButton = document.getElementById('recordButton');
        const playButton = document.getElementById('playButton');
        
        // Initialize WaveSurfer
        const wavesurfer = WaveSurfer.create({
            container: '#waveform',
            waveColor: '#4F4A85',
            progressColor: '#383351',
            cursorColor: '#383351',
            barWidth: 3,
            barRadius: 3,
            responsive: true,
            height: 50,
            normalize: true,
        });

        let conversationHistory = [
            {"role": "system", "content": "You are a helpful assistant."}
        ];

        recordButton.addEventListener('click', async () => {
            if (!mediaRecorder || mediaRecorder.state === 'inactive') {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);

                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    audioChunks = [];
                    const audioUrl = URL.createObjectURL(audioBlob);
                    
                    // Load audio into wavesurfer
                    wavesurfer.loadBlob(audioBlob);
                    
                    recordButton.textContent = 'Record';
                    recordButton.classList.remove('recording');

                    await sendAudioToServer(audioBlob);
                };

                mediaRecorder.start();
                recordButton.textContent = 'Stop Recording';
                recordButton.classList.add('recording');
            } else {
                mediaRecorder.stop();
            }
        });

        playButton.addEventListener('click', () => {
            if (wavesurfer.isPlaying()) {
                wavesurfer.pause();
                playButton.textContent = 'Play';
            } else {
                wavesurfer.play();
                playButton.textContent = 'Pause';
            }
        });

        document.getElementById('transcribeButton').addEventListener('click', async () => {
            if (wavesurfer.backend.buffer) {
                // Convert AudioBuffer to Blob
                const audioData = wavesurfer.backend.buffer;
                const audioBlob = await audioBufferToWav(audioData);
                await sendAudioToServer(audioBlob);
            } else {
                alert('Please record or load audio first');
            }
        });

        // Helper function to convert AudioBuffer to WAV Blob
        function audioBufferToWav(buffer) {
            return new Promise(resolve => {
                const numberOfChannels = buffer.numberOfChannels;
                const sampleRate = buffer.sampleRate;
                const wavesurferBlob = wavesurfer.getDecodedData();
                resolve(new Blob([wavesurferBlob], { type: 'audio/wav' }));
            });
        }

        async function sendAudioToServer(audioBlob) {
            const formData = new FormData();
            formData.append('file', audioBlob, 'recording.wav');

            try {
                // First, get the transcription
                const transcriptionResponse = await fetch('/transcribe', {
                    method: 'POST',
                    body: formData
                });

                const transcriptionResult = await transcriptionResponse.json();
                
                // Display user's message
                const chatMessages = document.getElementById('chatMessages');
                const userMessageDiv = document.createElement('div');
                userMessageDiv.className = 'message user-message';
                userMessageDiv.textContent = transcriptionResult.transcription;
                chatMessages.appendChild(userMessageDiv);

                // Add user message to conversation history
                conversationHistory.push({
                    "role": "user",
                    "content": transcriptionResult.transcription
                });

                // Then, get the chat completion with full conversation history
                const chatResponse = await fetch('/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        message: transcriptionResult.transcription,
                        conversation_history: conversationHistory
                    })
                });

                const chatResult = await chatResponse.json();
                
                // Add assistant's response to conversation history
                conversationHistory.push({
                    "role": "assistant",
                    "content": chatResult.response
                });

                // Display assistant's response with formatting
                const assistantMessageDiv = document.createElement('div');
                assistantMessageDiv.className = 'message assistant-message';
                
                // Convert markdown-style code blocks and format text
                const formattedText = chatResult.response
                    .replace(/```(\w+)?\n([\s\S]*?)```/g, (match, language, code) => {
                        return `<pre><code class="language-${language || ''}">${code.trim()}</code></pre>`;
                    })
                    .replace(/\n/g, '<br>');  // Convert newlines to <br> tags
                
                assistantMessageDiv.innerHTML = formattedText;
                chatMessages.appendChild(assistantMessageDiv);

                // Play the response audio if available
                if (chatResult.audio_url) {
                    const audio = new Audio(chatResult.audio_url);
                    audio.play();
                }

                // Scroll to bottom of chat
                chatMessages.scrollTop = chatMessages.scrollHeight;

                // Optional: Limit conversation history to prevent too long contexts
                if (conversationHistory.length > 10) {
                    // Keep system message and last 9 messages
                    conversationHistory = [
                        conversationHistory[0],
                        ...conversationHistory.slice(-9)
                    ];
                }
            } catch (error) {
                console.error('Error:', error);
                alert('Error processing audio');
            }
        }
    </script>
</body>
</html>